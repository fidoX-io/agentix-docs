---
title: 'Performance Benchmarks'
description: 'Performance metrics, capacity planning, and benchmarking data for Agentix deployments'
icon: 'chart-line'
---

# Performance Benchmarks

Understanding Agentix performance characteristics for capacity planning, optimization, and infrastructure sizing decisions.

## Benchmark Overview

### Test Environment Specifications

<Tabs>
<Tab title="Small Scale Test">
**Infrastructure:**
- **Server**: 8 vCPU, 16GB RAM, NVMe SSD
- **Database**: PostgreSQL 15, dedicated 4GB
- **Cache**: Redis 7, 2GB memory
- **Network**: 10Gbps connection

**Test Parameters:**
- **Duration**: 30 minutes sustained load
- **Ramp-up**: 5 minutes gradual increase
- **Monitoring**: 1-second granularity
</Tab>

<Tab title="Medium Scale Test">
**Infrastructure:**
- **Server**: 16 vCPU, 32GB RAM, NVMe SSD
- **Database**: PostgreSQL 15, dedicated 8GB
- **Cache**: Redis 7, 4GB memory
- **Network**: 10Gbps connection

**Test Parameters:**
- **Duration**: 60 minutes sustained load
- **Ramp-up**: 10 minutes gradual increase
- **Peak testing**: 15 minutes at maximum load
</Tab>

<Tab title="Large Scale Test">
**Infrastructure:**
- **Servers**: 3x 32 vCPU, 64GB RAM, NVMe SSD
- **Database**: PostgreSQL 15 cluster, 16GB
- **Cache**: Redis 7 cluster, 12GB memory
- **Network**: 25Gbps connection

**Test Parameters:**
- **Duration**: 120 minutes sustained load
- **Ramp-up**: 20 minutes gradual increase
- **Peak testing**: 30 minutes at maximum load
</Tab>
</Tabs>

## API Performance Benchmarks

### REST API Throughput

<CardGroup cols={2}>
<Card title="Small Deployment" icon="server">
**Sustained Performance:**
- **Requests/sec**: 2,500
- **Avg Response Time**: 45ms
- **95th Percentile**: 120ms
- **99th Percentile**: 250ms
- **Error Rate**: &lt;0.1%

**Peak Performance:**
- **Requests/sec**: 3,500
- **Avg Response Time**: 65ms
- **95th Percentile**: 180ms
- **99th Percentile**: 400ms
</Card>

<Card title="Medium Deployment" icon="rocket">
**Sustained Performance:**
- **Requests/sec**: 7,500
- **Avg Response Time**: 35ms
- **95th Percentile**: 95ms
- **99th Percentile**: 200ms
- **Error Rate**: &lt;0.05%

**Peak Performance:**
- **Requests/sec**: 12,000
- **Avg Response Time**: 55ms
- **95th Percentile**: 140ms
- **99th Percentile**: 300ms
</Card>
</CardGroup>

<CardGroup cols={1}>
<Card title="Large Deployment" icon="network-wired">
**Sustained Performance:**
- **Requests/sec**: 25,000
- **Avg Response Time**: 25ms
- **95th Percentile**: 75ms
- **99th Percentile**: 150ms
- **Error Rate**: &lt;0.02%

**Peak Performance:**
- **Requests/sec**: 40,000
- **Avg Response Time**: 40ms
- **95th Percentile**: 110ms
- **99th Percentile**: 250ms
</Card>
</CardGroup>

### API Endpoint Performance

<AccordionGroup>
<Accordion title="Authentication Endpoints">
**POST /api/auth/login**
- **Avg Response**: 150ms (includes password hashing)
- **95th Percentile**: 300ms
- **Throughput**: 500 req/sec per core

**POST /api/auth/refresh** 
- **Avg Response**: 25ms
- **95th Percentile**: 60ms
- **Throughput**: 2,000 req/sec per core

**GET /api/auth/user**
- **Avg Response**: 15ms
- **95th Percentile**: 40ms  
- **Throughput**: 3,000 req/sec per core
</Accordion>

<Accordion title="Job Management Endpoints">
**GET /api/jobs**
- **Avg Response**: 35ms (with pagination)
- **95th Percentile**: 85ms
- **Throughput**: 1,500 req/sec per core

**POST /api/jobs**
- **Avg Response**: 125ms (includes validation)
- **95th Percentile**: 250ms
- **Throughput**: 400 req/sec per core

**GET /api/jobs/{id}**
- **Avg Response**: 20ms
- **95th Percentile**: 50ms
- **Throughput**: 2,500 req/sec per core
</Accordion>

<Accordion title="Agent Interaction Endpoints">
**POST /api/agents/chat**
- **Avg Response**: 200ms (excluding AI processing)
- **95th Percentile**: 400ms
- **Throughput**: 300 req/sec per core

**GET /api/agents/{id}/status**
- **Avg Response**: 12ms
- **95th Percentile**: 30ms
- **Throughput**: 4,000 req/sec per core

**POST /api/agents/tools/execute**
- **Avg Response**: 500ms (varies by tool)
- **95th Percentile**: 1,200ms
- **Throughput**: 100 req/sec per core
</Accordion>
</AccordionGroup>

## Real-time Communication Performance

### WebSocket Connection Metrics

<CardGroup cols={2}>
<Card title="Concurrent Connections" icon="users">
**Small Deployment:**
- **Max Connections**: 2,000
- **Connection Setup**: 50ms avg
- **Memory per Connection**: 2KB
- **CPU Overhead**: 0.05% per 100 connections

**Medium Deployment:**
- **Max Connections**: 8,000
- **Connection Setup**: 35ms avg
- **Memory per Connection**: 1.8KB
- **CPU Overhead**: 0.03% per 100 connections

**Large Deployment:**
- **Max Connections**: 25,000+
- **Connection Setup**: 25ms avg
- **Memory per Connection**: 1.5KB
- **CPU Overhead**: 0.02% per 100 connections
</Card>

<Card title="Message Throughput" icon="comments">
**Small Deployment:**
- **Messages/sec**: 50,000
- **Avg Latency**: 5ms
- **95th Percentile**: 15ms
- **Max Message Size**: 1MB

**Medium Deployment:**
- **Messages/sec**: 200,000
- **Avg Latency**: 3ms
- **95th Percentile**: 10ms
- **Max Message Size**: 1MB

**Large Deployment:**
- **Messages/sec**: 500,000+
- **Avg Latency**: 2ms
- **95th Percentile**: 8ms
- **Max Message Size**: 1MB
</Card>
</CardGroup>

### Voice Communication Performance

<CardGroup cols={2}>
<Card title="Audio Processing" icon="microphone">
**Concurrent Calls (Small):**
- **Max Calls**: 100
- **Audio Latency**: 45ms avg
- **Jitter**: &lt;5ms
- **CPU per Call**: 1.2%

**Concurrent Calls (Medium):**
- **Max Calls**: 400
- **Audio Latency**: 35ms avg
- **Jitter**: &lt;3ms
- **CPU per Call**: 0.8%

**Concurrent Calls (Large):**
- **Max Calls**: 1,500+
- **Audio Latency**: 28ms avg
- **Jitter**: &lt;2ms
- **CPU per Call**: 0.5%
</Card>

<Card title="Audio Quality" icon="volume-high">
**Codec Performance:**
- **Opus 48kHz**: Best quality, 1.5x CPU
- **Opus 16kHz**: Balanced, 1x CPU
- **G.711**: Low CPU, 0.6x CPU

**Quality Metrics:**
- **MOS Score**: 4.2/5.0 (Opus 48kHz)
- **Packet Loss**: &lt;0.1%
- **Bandwidth**: 32-64 kbps per call
- **Buffer Size**: 20ms default
</Card>
</CardGroup>

## Database Performance

### PostgreSQL Metrics

<CardGroup cols={2}>
<Card title="Query Performance" icon="database">
**Simple Queries (SELECT/INSERT):**
- **Avg Latency**: 2ms
- **95th Percentile**: 8ms
- **Throughput**: 15,000 TPS

**Complex Queries (JOIN/aggregation):**
- **Avg Latency**: 15ms
- **95th Percentile**: 45ms
- **Throughput**: 2,000 TPS

**Write Operations:**
- **INSERT**: 1ms avg, 12,000 TPS
- **UPDATE**: 3ms avg, 8,000 TPS
- **DELETE**: 2ms avg, 10,000 TPS
</Card>

<Card title="Connection Management" icon="plug">
**Connection Pool (Small):**
- **Max Connections**: 100
- **Pool Size**: 20
- **Connection Latency**: 0.5ms

**Connection Pool (Medium):**
- **Max Connections**: 200
- **Pool Size**: 50
- **Connection Latency**: 0.3ms

**Connection Pool (Large):**
- **Max Connections**: 500
- **Pool Size**: 100
- **Connection Latency**: 0.2ms
</Card>
</CardGroup>

### Database Growth Patterns

<AccordionGroup>
<Accordion title="Storage Growth Rates">
**User Data:**
- **Per User**: ~50MB/month average
- **Profiles**: 2KB per user
- **Job History**: 10KB per job
- **Message History**: 5KB per conversation

**System Data:**
- **Logs**: 1GB/month per 1,000 users
- **Analytics**: 500MB/month per 1,000 users
- **Backups**: 3x primary data size

**Optimization:**
- **Compression**: 40% storage reduction
- **Partitioning**: 60% query performance improvement
- **Archival**: 70% active data reduction
</Accordion>

<Accordion title="Index Performance">
**Primary Keys:**
- **Lookup**: &lt;0.1ms
- **Insert**: &lt;0.5ms

**Foreign Keys:**
- **Lookup**: &lt;0.5ms
- **Join**: &lt;2ms

**Full Text Search:**
- **Simple**: 5ms avg
- **Complex**: 25ms avg
- **Index Size**: 15% of text data

**Composite Indexes:**
- **2 columns**: &lt;1ms
- **3+ columns**: &lt;3ms
</Accordion>
</AccordionGroup>

## Cache Performance

### Redis Performance Metrics

<CardGroup cols={2}>
<Card title="Memory Operations" icon="bolt">
**Basic Operations:**
- **GET**: 0.1ms avg, 100K ops/sec
- **SET**: 0.15ms avg, 80K ops/sec
- **DEL**: 0.1ms avg, 90K ops/sec

**Complex Operations:**
- **HGETALL**: 0.3ms avg, 30K ops/sec
- **ZADD**: 0.2ms avg, 50K ops/sec
- **LPUSH**: 0.15ms avg, 60K ops/sec

**Batch Operations:**
- **MGET**: 0.5ms avg, 20K ops/sec
- **PIPELINE**: 2ms avg, 5K ops/sec
</Card>

<Card title="Cache Hit Rates" icon="target">
**Session Data:**
- **Hit Rate**: 98.5%
- **Avg TTL**: 30 minutes
- **Size**: 2KB per session

**API Responses:**
- **Hit Rate**: 85%
- **Avg TTL**: 5 minutes
- **Size**: 5KB per response

**User Profiles:**
- **Hit Rate**: 95%
- **Avg TTL**: 60 minutes
- **Size**: 1KB per profile
</Card>
</CardGroup>

## Resource Utilization

### CPU Usage Patterns

<CodeGroup>
```bash CPU Utilization (% of total)
# Small Deployment (8 cores)
Idle Load:        15%
Normal Load:      45%
Peak Load:        75%
Max Sustainable:  85%

# Medium Deployment (16 cores)  
Idle Load:        8%
Normal Load:      35%
Peak Load:        65%
Max Sustainable:  80%

# Large Deployment (32 cores)
Idle Load:        5%
Normal Load:      25%
Peak Load:        55%
Max Sustainable:  75%
```

```bash CPU Breakdown by Component
# Typical load distribution
Agentix Core:     40%
RTC Processing:   30%
Database:         20%
Redis:           5%
System:          5%
```

```bash CPU Scaling Efficiency
# Cores vs Performance (requests/sec)
2 cores:   1,200 req/sec
4 cores:   2,800 req/sec  (2.3x scaling)
8 cores:   6,000 req/sec  (2.1x scaling)
16 cores: 12,500 req/sec  (2.1x scaling)
32 cores: 24,000 req/sec  (1.9x scaling)
```
</CodeGroup>

### Memory Usage Patterns

<CodeGroup>
```bash Memory Utilization (GB)
# Small Deployment (16GB total)
Agentix Core:     4GB
PostgreSQL:       4GB  
Redis:           2GB
System Cache:     3GB
Available:        3GB

# Medium Deployment (32GB total)
Agentix Core:     8GB
PostgreSQL:       8GB
Redis:           4GB
System Cache:     6GB
Available:        6GB

# Large Deployment (64GB total)  
Agentix Core:    16GB
PostgreSQL:      16GB
Redis:           8GB
System Cache:    12GB
Available:       12GB
```

```bash Memory Growth Patterns
# Per 1,000 active users
Agentix Core:   +500MB
Session Cache:  +200MB
Connection Pool: +100MB
Temporary Data:  +150MB
Total Growth:   ~950MB per 1,000 users
```
</CodeGroup>

### Network Utilization

<CardGroup cols={2}>
<Card title="Bandwidth Usage" icon="wifi">
**API Traffic:**
- **Avg**: 50 Mbps per 1,000 users
- **Peak**: 200 Mbps per 1,000 users
- **WebSocket**: 25 Mbps per 1,000 connections

**Voice Traffic:**
- **Per Call**: 64 kbps (Opus codec)
- **100 Calls**: 6.4 Mbps
- **1,000 Calls**: 64 Mbps

**Database Replication:**
- **Avg**: 10 Mbps
- **Peak**: 100 Mbps
- **Backup**: 500 Mbps (during backup window)
</Card>

<Card title="Connection Metrics" icon="plug">
**TCP Connections:**
- **API**: 2,000 max per core
- **WebSocket**: 8,000 max per core
- **Database**: 100 max per instance

**Connection Pool:**
- **Efficiency**: 85% utilization
- **Reuse Rate**: 95%
- **Timeout**: 30 seconds idle

**Load Distribution:**
- **Round Robin**: Even distribution
- **Least Connections**: 5% better performance
- **IP Hash**: Session persistence
</Card>
</CardGroup>

## Capacity Planning

### User Scaling Guidelines

<AccordionGroup>
<Accordion title="Small Scale (Up to 1,000 users)">
**Infrastructure Requirements:**
- **CPU**: 8 cores minimum
- **RAM**: 16GB minimum
- **Storage**: 500GB (includes growth)
- **Network**: 5Gbps connection

**Expected Performance:**
- **Concurrent Users**: 500
- **API Requests**: 2,500/sec
- **Voice Calls**: 100 simultaneous
- **Response Time**: &lt;100ms (95th percentile)

**Growth Considerations:**
- **Storage**: 50GB/month
- **Bandwidth**: Linear scaling
- **CPU**: Vertical scaling preferred
</Accordion>

<Accordion title="Medium Scale (1,000 - 5,000 users)">
**Infrastructure Requirements:**
- **CPU**: 16-32 cores
- **RAM**: 32-64GB
- **Storage**: 1-2TB (includes growth)
- **Network**: 10Gbps connection

**Expected Performance:**
- **Concurrent Users**: 2,500
- **API Requests**: 12,500/sec
- **Voice Calls**: 500 simultaneous
- **Response Time**: &lt;75ms (95th percentile)

**Growth Considerations:**
- **Storage**: 250GB/month
- **Bandwidth**: Near-linear scaling
- **CPU**: Horizontal scaling recommended
</Accordion>

<Accordion title="Large Scale (5,000+ users)">
**Infrastructure Requirements:**
- **CPU**: 64+ cores (distributed)
- **RAM**: 128GB+ (distributed)
- **Storage**: 5TB+ (distributed)
- **Network**: 25Gbps+ connection

**Expected Performance:**
- **Concurrent Users**: 12,500+
- **API Requests**: 50,000+/sec
- **Voice Calls**: 2,000+ simultaneous
- **Response Time**: &lt;50ms (95th percentile)

**Growth Considerations:**
- **Storage**: 1TB+/month
- **Bandwidth**: Requires load balancing
- **CPU**: Kubernetes recommended
</Accordion>
</AccordionGroup>

### Performance Optimization

<CodeGroup>
```yaml Optimization Checklist
Database:
  - Connection pooling: ✓
  - Query optimization: ✓
  - Index tuning: ✓
  - Partitioning: ✓
  - Read replicas: ✓

Caching:
  - Redis clustering: ✓
  - Cache warming: ✓
  - TTL optimization: ✓
  - Compression: ✓

Application:
  - Code profiling: ✓
  - Memory optimization: ✓
  - Async processing: ✓
  - Load balancing: ✓

Infrastructure:
  - SSD storage: ✓
  - Network optimization: ✓
  - CPU affinity: ✓
  - Monitoring: ✓
```

```bash Performance Tuning Commands
# Database optimization
VACUUM ANALYZE;
REINDEX DATABASE agentix_production;
UPDATE pg_stat_statements SET calls = 0, total_time = 0;

# Redis optimization  
BGREWRITEAOF
MEMORY USAGE <key>
INFO memory

# System optimization
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'vm.swappiness = 10' >> /etc/sysctl.conf
sysctl -p
```

```yaml Monitoring KPIs
Response Time:
  - API endpoints: &lt; 100ms (95th percentile)
  - WebSocket: &lt; 50ms
  - Voice latency: &lt; 30ms

Throughput:
  - API requests: > 10,000/sec
  - WebSocket messages: > 100,000/sec
  - Database queries: > 5,000 TPS

Resource Usage:
  - CPU: &lt; 80% sustained
  - Memory: &lt; 85% utilization
  - Disk I/O: &lt; 80% utilization
  - Network: &lt; 70% capacity

Availability:
  - Uptime: > 99.9%
  - Error rate: &lt; 0.1%
  - Recovery time: &lt; 60 seconds
```
</CodeGroup>

## Benchmark Testing Tools

### Load Testing Scripts

<CodeGroup>
```javascript API Load Test (Artillery)
config:
  target: 'https://your-agentix-instance.com'
  phases:
    - duration: 300  # 5 minutes ramp-up
      arrivalRate: 10
      rampTo: 100
    - duration: 1800  # 30 minutes sustained
      arrivalRate: 100
    - duration: 300   # 5 minutes ramp-down
      arrivalRate: 100
      rampTo: 10
  
scenarios:
  - name: "API Endpoints"
    weight: 70
    flow:
      - post:
          url: "/api/auth/login"
          json:
            email: "test@example.com"
            password: "testpassword"
          capture:
            - json: "$.token"
              as: "token"
      - get:
          url: "/api/jobs"
          headers:
            Authorization: "Bearer {{ token }}"
      - post:
          url: "/api/jobs"
          headers:
            Authorization: "Bearer {{ token }}"
          json:
            title: "Test Job"
            description: "Load test job"
  
  - name: "WebSocket Connections"
    weight: 30
    engine: ws
    flow:
      - connect:
          url: "wss://your-agentix-instance.com/ws"
          headers:
            Authorization: "Bearer {{ token }}"
      - send:
          payload: '{"type": "ping", "data": {}}'
      - think: 5
      - send:
          payload: '{"type": "message", "data": {"text": "Hello"}}'
```

```python Database Performance Test
#!/usr/bin/env python3
import asyncio
import asyncpg
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

async def benchmark_database():
    """Benchmark database performance"""
    
    # Connection pool
    pool = await asyncpg.create_pool(
        "postgresql://user:password@localhost/agentix_production",
        min_size=10,
        max_size=50
    )
    
    # Test queries
    queries = [
        ("Simple SELECT", "SELECT id FROM users LIMIT 100"),
        ("JOIN Query", """
            SELECT u.id, u.name, j.title 
            FROM users u 
            JOIN jobs j ON u.id = j.user_id 
            LIMIT 100
        """),
        ("Complex Aggregation", """
            SELECT 
                DATE_TRUNC('hour', created_at) as hour,
                COUNT(*) as count,
                AVG(duration) as avg_duration
            FROM jobs 
            WHERE created_at > NOW() - INTERVAL '24 hours'
            GROUP BY hour
            ORDER BY hour
        """)
    ]
    
    results = {}
    
    for name, sql in queries:
        print(f"Testing {name}...")
        times = []
        
        for i in range(100):
            start = time.time()
            async with pool.acquire() as conn:
                await conn.fetch(sql)
            end = time.time()
            times.append((end - start) * 1000)  # Convert to ms
        
        results[name] = {
            'avg': statistics.mean(times),
            'median': statistics.median(times),
            'p95': statistics.quantiles(times, n=20)[18],  # 95th percentile
            'p99': statistics.quantiles(times, n=100)[98]  # 99th percentile
        }
    
    # Print results
    for name, metrics in results.items():
        print(f"\n{name} Results:")
        print(f"  Average: {metrics['avg']:.2f}ms")
        print(f"  Median:  {metrics['median']:.2f}ms")  
        print(f"  95th %:  {metrics['p95']:.2f}ms")
        print(f"  99th %:  {metrics['p99']:.2f}ms")
    
    await pool.close()

if __name__ == "__main__":
    asyncio.run(benchmark_database())
```

```bash System Resource Monitor
#!/bin/bash
# monitor.sh - System resource monitoring during load tests

LOG_FILE="performance_$(date +%Y%m%d_%H%M%S).log"
INTERVAL=5  # seconds

echo "Starting performance monitoring..."
echo "Timestamp,CPU%,Memory%,DiskIO%,NetworkRX,NetworkTX" > $LOG_FILE

while true; do
    TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')
    
    # CPU usage
    CPU=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    
    # Memory usage  
    MEMORY=$(free -m | awk 'NR==2{printf "%.1f", $3*100/($3+$4)}')
    
    # Disk I/O (%)
    DISKIO=$(iostat -x 1 2 | tail -n +4 | awk '{sum+=$10} END {printf "%.1f", sum/NR}')
    
    # Network (bytes/sec)
    NET_RX=$(cat /sys/class/net/eth0/statistics/rx_bytes)
    NET_TX=$(cat /sys/class/net/eth0/statistics/tx_bytes)
    
    echo "$TIMESTAMP,$CPU,$MEMORY,$DISKIO,$NET_RX,$NET_TX" >> $LOG_FILE
    
    sleep $INTERVAL
done
```
</CodeGroup>

## Next Steps

<CardGroup cols={2}>
<Card title="Running Locally" icon="play" href="/developer-guide/self-hosting/overview">
  Start with local deployment to test performance characteristics in your environment
</Card>

<Card title="Network Configuration" icon="shield" href="/developer-guide/self-hosting/network-firewall">
  Optimize network settings and firewall rules for maximum performance
</Card>
</CardGroup>